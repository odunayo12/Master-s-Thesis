\chapter{The Euler-Poisson scheme}
\section{Preliminaries and Notations}
Throughout this thesis, we will always assume given a \emph{complete} probability space \break $(\Omega , \mathcal{F}, (\mathcal{F}_t)_{t \geq 0}, \mathcal{P})$, i.e., the $\sigma$-algebra  $\mathcal{F}$ contains additionally all subsets of nullsets. We also assume a filtration $(\mathcal{F}_t)_{t \geq 0}$, which is a family of $\sigma$-algebras such that $\mathcal{F}_s \subset \mathcal{F}_t$ for all $s \leq t$. Our filtration is assumed to satisfy the usual hypothesis, i.e., $\mathcal{F}_0$ contains all $\mathcal{P}$-nullsets of $\mathcal{F}$, and the filtration is right continuous. By $\mathcal{G}_t = \mathcal{F}_t \bigvee \mathcal{H}$ we mean $\mathcal{G}_t$ is a filtration generated by the union of $\mathcal{F}_t $ and $ \mathcal{H}$.\\
Furthermore, we will denote by $\mathbb{R}$ the set of all real numbers, and $\mathbb{R}_+$ the set of all positive real numbers. Equality in distribution is denoted by $\overset{d}{=}$ .
We will employ the notation $|\cdot|$, as used by \morecite{ferreiro2016euler}, to indistinctly denote the Euclidean norm for vectors or the Frobenius norm of matrices. For $x,y \in \mathbb{R}$, $(x \land y) := \min \{x,\, y\}$ and $(x \lor y) := \max \{x,\, y\}$. $\mathcal{C}^n$ is the Space of $n$ times continuously differentiable functions and $\mathcal{C}^{1,n}$ is the Space of functions which are continuously differentiable with respect to the first variable, and n times continuously differentiable with respect to the second variable. $L^p$ denotes the Space of measurable functions with a finite $p$-th norm. Finally, $\langle \cdot, \cdot \rangle$ denotes inner product.

\section{Overview of L\'evy Processes}\label{Section_Levy}
In this section we give the definition of a L\'evy process and closely related distributions useful for the study of its characteristics as well as performing numerical analysis. We refer the reader to \morecite{sato1999levy} for detailed treatment of general L\'evy Processes and to \morecite{tankov2003financial} for their applications in financial modelling.
\begin{definition}[L\'evy process] A d-dimensional adapted stochastic process \break $X = (X_t)_{t \in [0, \, T]}$ on $(\Omega , \mathcal{F}, (\mathcal{F}_t)_{t \geq 0}, \mathcal{P})$, is a called a L\'evy process if the following conditions are satisfied.
\begin{itemize}
    \item $X_0 = 0 $ a.s.,
    \item X is a.s. c\`adl\`ag (i.e. right-continuous with left limits),
    \item X has independent increments, i.e., for all $ n \in \mathbb{N} $ and all sequences \break $0 = t_0 \leq t_1 \leq \cdots \leq t_n < \infty $, $X_{t_1} - X_{t_0}, \, \ldots, X_{t_n} - X_{t_{n-1}}$ are independent,
    \item X has stationary increments, i.e.,  for all $ s,t \in [0, \, T], \, s<t$, $ X_t - X_s \overset{d}{=} X_{t-s}$.
    %\item Y is stochastiscally continuous, i.e. $\forall A>0 $, $$ \lim_{s \to t , s<t}\mathcal{P} (|Y_t - Y_s |>A) =0$$
\end{itemize}
\end{definition}
Given the properties listed above, we consider an $\mathbb{R}^d$-valued adapted stochastic process $Y= (Y_t)_{t \in [0, T]}$ defined on the filtered probability space $(\Omega , \mathcal{F}, (\mathcal{F}_t)_{t \geq 0}, \mathcal{P})$, which is a strong solution to
\begin{equation}\label{eq_major}
      Y_t = y_0  + \int_0^t a(Y_{s-})dX_s \qquad t \in [0, \, T],
\end{equation}
where $T < \infty$, $y_0  \in \mathbb{R}^d$ is the deterministic initial value; $a : \mathbb{R}^d \to \mathbb{R}^{d \times d}
$ is a coefficient function, on which we impose the standard Lipschitz assumption to ensure the  existence of a unique strong  solution. The smoothness of $a$  is specified in the sequel. \break  $X=(X_t)_{t\in [0,T]}$ is a $d$-dimensional L\'evy process. We write ${Y}_{t-}$ instead of ${Y}_{t}$ in order that the integrand be predictable, and that it is well-defined as an It\^o integral with $${Y}_{t-} = \lim_{s \uparrow t}Y_s.$$
A very crucial tool to the analysis of distributions of L\'evy Process is the so called characteristic function, i.e. the Fourier transform of a probability distribution.
\begin{definition}[Characteristic Function] The characteristic function $\tilde{\Phi}(\vartheta)$ of a probability measure on $\mathbb{R}^d$ is
\begin{equation}
    \tilde{\Phi}(\vartheta) = \int_{\mathbb{R}^d} e^{i\langle \vartheta,x \rangle} \Phi(dx), \qquad \vartheta \in \mathbb{R}^d.
\end{equation}
Similarly, the characteristic function of the distribution $\mathcal{P}_X$ of a random variable X on $\mathbb{R}^d$ denoted by $\tilde{\mathcal{P}}_X(\vartheta): \mathbb{R}^d \rightarrow  \mathbb{R} $ is given as 
\begin{equation}
\tilde{\mathcal{P}}_X(\vartheta) =  \int_{\mathbb{R}^d} e^{i\langle \vartheta,x \rangle}\mathcal{P}_X(dx) = \mathbb{E}[e^{i\langle \vartheta,X \rangle}].
\end{equation}
\end{definition}
Furthermore, another very important notion for the study of L\'evy processes is that of infinite divisibility of a distribution.
\begin{definition}
Let $\tilde{\mathcal{P}}^n$ denote the n-fold convolution of a probability measure $\tilde{\mathcal{P}}$ with itself. A probability measure $\tilde{\mathcal{P}}$ on  $\mathbb{R}^d$ is infinitely divisible if for all  $n \in \mathbb{N}$, there is a probability measure $\tilde{\mathcal{P}}_n$ on $\mathbb{R}^d$
such that, $$\tilde{\mathcal{P}}^n_n := \underbrace{\tilde{\mathcal{P}}_n * \ldots *\tilde{\mathcal{P}}_n}_\text{n times} = \tilde{\mathcal{P}}. $$ 
\end{definition}
The following is the famous $\emph{L\'evy-Khintchine formula} $ which gives a representation of the characteristic function of all infinitely divisible distributions.
\begin{theorem}[Theorem 8.2 p. 38, \morecite{sato1999levy}] \begin{enumerate}[label=(\roman*)]\label{Thm_LK}
    \item[]
    \item if $\mathcal{P}$ is an infinitely divisible distribution on $\mathbb{R}^d$, then
    \begin{equation}\label{inf_div_dist}
        \tilde{\mathcal{P}}(\vartheta) = \exp \bigg[-\frac{1}{2}\langle \vartheta, \Sigma \vartheta \rangle + i\langle a,\vartheta \rangle + \int_{\mathbb{R}^d} \big( e^{i\langle \vartheta,x \rangle } - 1 - {i\langle \vartheta,x \rangle } \mathbbm{1}_{\{||x||\leq1\}} \big) v(dx) \bigg],
    \end{equation}
    where $a \in \mathbb{R}^d$, $\Sigma \in \mathbb{R}^{d \times d}$ is a symmetric and non-negative definite matrix and v is a measure on $\mathbb{R}^d$ satisfying 
    \begin{equation}\label{cond_inf_div_dist}
        v(\{0\}) = 0 \qquad and \qquad \int_{\mathbb{R}^d} (1 \land |x|^2)  v(dx)< \infty.
    \end{equation}
    \item The representation of $\tilde{\mathcal{P}}$ in (\ref{inf_div_dist}) by $a, \, \Sigma \, and \, v$ is unique.
    \item Conversely, if $a \in \mathbb{R}^d$, $\Sigma \in \mathbb{R}^{d \times d}$ is symmetric and non-negative definite matrix, v is a measure on  $\mathbb{R}^d$ satisfying (\ref{cond_inf_div_dist}), then there is an infinitely divisible distribution $ \mathcal{P}$ whose characteristic function is given by (\ref{inf_div_dist}).
\end{enumerate}
\end{theorem}
\begin{definition}
The triplet $(a, \, \Sigma, \, v)$ in Theorem \ref{Thm_LK} is called generating triplet. Moreover, if $\Sigma = 0$, $\mathcal{P}$ is called a pure jump L\'evy process.
\end{definition}

The following gives an explicit representation of the characteristic function of the law of a L\'evy process.
\begin{proposition}[Characteristic function of a L\'evy process]
For any L\'evy process $(Y_t)_{t \in \mathbb{R}_+} in \mathbb{R}^d$, there exist a unique L\'evy triplet $(a, \, \Sigma, \, v)$ such that $\forall t > 0$, 
\begin{equation*}
    \mathbb{E}[e^{i\langle z, Y_t\rangle}] = e^{t\Psi(\vartheta)}, \qquad \forall \, \vartheta \in \mathbb{R}^d,
\end{equation*}
where
\begin{equation}
    \Psi(\vartheta) = -\frac{1}{2}\langle \vartheta, \Sigma  \vartheta \rangle + i\langle a,\vartheta \rangle + \int_{\mathbb{R}^d} \big( e^{i\langle \vartheta,x \rangle } - 1 - {i\langle \vartheta,x \rangle } \mathbbm{1}_{\{|x|\leq1\}} \big) v(dx).
\end{equation}
\end{proposition}
For the Euler-Poison Scheme, we assume that the marginals of the process are in $L^2 (\Omega , \mathcal{F}, \mathcal{P})$ hence omit the truncation function, leading to the  so called characteristic exponent of the L\'evy process expressed as 
\begin{equation*}
    \mathbb{E}[e^{i\langle \vartheta, X_t\rangle}] = e^{t\Psi(\vartheta)}, \qquad \forall \, \vartheta \in \mathbb{R}^d,
\end{equation*}
where
\begin{equation}\label{inf_div_dist_levy}
    \Psi(\vartheta) = -\frac{1}{2}\langle \vartheta, \Sigma \Sigma^T \vartheta \rangle + i\langle b, \vartheta \rangle + \int_{\mathbb{R}^d} \big( e^{i\langle \vartheta,x \rangle } - 1 - {i\langle \vartheta,x \rangle }\big) \, v(dx),
\end{equation}
and $b \in \mathbb{R}^d$, $\Sigma \in \mathbb{R}^{d \times d}$ and $v$ is
a measure concentrated on $\mathbb{R}^d \setminus \{0\}$ and \break $\int_{\mathbb{R}^d} (1 \land |x|^2)  v(dx)< \infty.$

\section{L\'evy-It\^o Decomposition}
Here we state the classical L\'evy-It\^o decomposition, which describes the structure of the sample path of a L\'evy process. It expresses the sample path of a L\'evy process as a sum of four independent parts-the drift, the Guassian part, the small jump part and the large jump part. In order to state this, we need the notion of a Poisson random measure (PRM). 
\begin{definition}
Let $G \subset \mathbb{R}^d$. A Radon measure on the space $(G, \, \mathcal{G})$ is a measure $\mu$ such that for every compact measurable set $B \in \mathcal{G}$, $\mu(B) < \infty$.
\end{definition}
\begin{definition}
Let $(\Omega , \mathcal{F}, \mathcal{P})$ be a probability space, $A \subset \mathbb{R}^d$ and $\mu$ a given Radon measure on $(G, \mathcal{G})$. A Poisson random measure on $G$ with intensity $\mu$ is an integer valued random measure 
$N: \Omega \times \mathcal{G} \to \mathbb{N}$, $(\omega, B) \mapsto N(\omega, B)$, such that
\begin{enumerate}[label=(\roman*)]
    \item For any bounded measurable set $B \subset G$, $N(B) < \infty$ is an integer valued random variable.
    \item For each measurable set $B \subset G$, $N(\omega, B) = N(B)$ is a PRM with parameter $\mu(B)$,
    \begin{equation}
        \mathcal{P}(N(B) = \kappa) = e^{-\mu(B)}\dfrac{(\mu(B))^\kappa}{\kappa!}, \qquad \qquad \forall \kappa \in \mathbb{N}.
    \end{equation}
    \item For disjoint measurable sets $B_1, \ldots , B_n \in \mathcal{G}$, the variables $N(B_1), \ldots , N(B_n)$ are independent.
\end{enumerate}
Moreover, $\hat{N}(B) = N(B) - \mu(B)$ is called the compensated PRM.
\end{definition}
\begin{remark}
$N$ can be represented through Dirac masses located at random points  \break $(V_i)_{i\geq1}, \, V_i: \Omega \to A$ given by $$N(B) = \sum_i \delta_{V_i}(B),$$ provided $B \cap (V_i)_{i\geq1} < \infty$ for any compact set $B \subset G$
\end{remark}
The following allows one to construct a PRM from a given Radon measure.
\begin{proposition}
For any Radon measure $\mu$ on $G \subset \mathbb{R}^d$, there exists a PRM $N$ on $G$ with intensity $\mu$. 
\end{proposition}
\begin{proof}
See (Section 2.6, \morecite{tankov2003financial}).
\end{proof}
The process stated below corresponds to the PRM and its compensated counterpart
\begin{proposition}
Let $N$ be an adapted PRM on $G = [0, \, T] \times \mathbb{R}^d \setminus \{0\}$ with intensity $\mu$ with compensated PRM $\hat{N} = N - \mu$, and $f : G \to \mathbb{R}^d$  such that $\mu(f) < \infty$. Then the process
\begin{equation}
    \begin{split}
        \tilde{X}_t &= \int^t_0 \int_{\mathbb{R}^d \setminus \{0\}} f(s, y) \hat{N}(ds dy)\\
        &= \int^t_0 \int_{\mathbb{R}^d \setminus \{0\}} f(s, y) N(ds dy) - \int^t_0 \int_{\mathbb{R}^d \setminus \{0\}} f(s, y),
    \end{split}
\end{equation}
is a martingale.
\end{proposition}
\begin{definition}[Jump Measure]
Let $V$ be  an $\mathbb{R}^d-$valued c\`adl\`ag process. The jump measure of $V$ is the random measure on $\mathcal{B}((0, \, \infty) \, \times \, \mathbb{R}^d)$ defined by 
\begin{equation}\label{eq_Jmeasure}
    N_V(A) = \# \{t \, ; \, \Delta V_t \neq 0 \, and  \, (t, \, \Delta V_t) \, \in A \}.
\end{equation}
\end{definition}

Thus the jump measure counts the number of jumps of $V$ of a particular size that falls into the set $A$. The L\'evy measure builds on this and is given as follows.

\begin{theorem}
Let $V$ be  $\mathbb{R}^d-$valued L\'evy process. The measure $v$  given by 
\begin{equation}
    v(A) = \mathbb{E}[\# \{t \, \in [0, 1]; \, \Delta V_t \neq 0 \, and  \, \Delta V_t \, \in A \}], \qquad A \, \in \, \mathcal{B}(\mathbb{R}^d),
\end{equation}
is called a L\'evy measure.
\end{theorem}
\begin{interpretation}
The L\'evy measure of a Borel set is equal to the expected number of jumps in a time interval $[0,\, 1]$ with jump sizes in the Borel set. The jumps are described by the PRM.
\end{interpretation}
The L\'evy-It\^o decomposition is stated in the following proposition.
\begin{proposition}[Theorem 1, p.11, L\'evy-It\^o decomposition \morecite{tankov2003financial}] \label{levy_Ito_decop}
Let $X = (X_t)_{t\geq0}$ be an $\mathbb{R}^d-$valued L\'evy process, with L\'evy measure $v$. Then 
\begin{enumerate}
    \item The jump measure $N_X$ (cf. (\ref{eq_Jmeasure})) of $X$ is a Poison random measure on $[0 \, \infty) \times \mathbb{R}^d$ with intensity $dt \times v$.
    \item The L\'evy measure $v$ satisfies $\int_{\mathbb{R}^d} (1 \land |x|^2)  v(dx)< \infty$.
    \item There exist $\gamma \, \in \, \mathbb{R}^d$ a $d$-dimensional Brownian Motion $W$ and covariance matrix $\Sigma$ such that 
    \begin{align}
         X_t &= \gamma t + \Sigma W_t + X^{(1)}_t + X^{(2)}_t \label{eq_215} \\ 
         \intertext{where}
        X^{(1)}_t &= \int_0^t \int_{|x| \geq 1} x N_X (ds \times dx)  \label{eq_216} \\ 
        X^{(2)}_t &=  \int_0^t \int_{|x| \in [\varepsilon \,, 1)} x (N_X (ds \times dx) - v (dx)ds) \label{eq_217} \\ 
        &=:  \int_0^t \int_{|x| \in [\varepsilon \,, 1)} x \tilde{N}_X (ds \times dx), \label{eq_218}
     \end{align}
     for $\varepsilon \in \mathbb{R}_+ $.
\end{enumerate}
The three terms are independent and the convergence in the last term is almost sure and uniform in $t$ on compacts. The triple $( \gamma,\, v,\, \Sigma)$ is called the characteristic triple of $X$.
\end{proposition}
\begin{proof}
See \morecite{tankov2003financial}.
\end{proof}
\begin{remark}
The mean value of $X_1$ exist if and only if (cf. Sato, 1999 p.39) $$\int_{|x| > 1}  |x| v(dx)< \infty,$$ in which case we can rewrite setting $\mathbb{E}[X_t]=: bt$ the representation (\ref{eq_215})-(\ref{eq_218}) as 
\begin{equation}\label{eq_219}
    X_t = bt + \Sigma W_t + \int_0^t \int_{\mathbb{R}^d} x \tilde{N}_X (ds \times dx),
\end{equation}
where the integral on the right-hand side of (\ref{eq_219}) is the compensated jump of $X$
\end{remark}

\begin{interpretation}The jumps of $X$ are contained in the discontinuous processes $ X^{(1)}_t$ and $ X^{(2)}_t$. While the sum $$ X^{(1)}_t = \sum_{s \in [0,t]} \Delta X_s \mathbbm{1}_{\{|\Delta X_s|\geq 1\}} $$ contains almost surely finite number of terms and is a well-defined compound Poisson process, the compensated jump integral $X^{(2)}_t$ is centered with $\varepsilon \to 0$ to avoid divergence since the jump measure $v$ can have singularity at 0. Moreover, $ X^{(2)}_t$ is a martingale by definition.
The implication of this is that, every L\'evy process can be approximated with arbitrary precision by the sum of a Brownian motion with drift and a compound Poisson process.
Additionally from \eqref{eq_215}, if we set $$A_t = bt + X^{(1)}_t$$ and $$M_t = \Sigma W_t + X^{(2)}_t,$$ it follows that every L\'evy process is a semimartingale. This is because $M$ is by definition a martingale while the condition $\int_{|x| > 1} v(dx)< \infty$ verifies that $X$ has a finite number of jumps with absolute value larger than 1 thus $A$ is of finite variation (see \morecite{barndorff2012basics} and the references provided therein).
\end{interpretation}


Denoting by $W=(W_t)_{t\in [0,T]} $ a $d$-dimensional Wiener process independent of $L$, where $L=(L_t)_{t\in [0,T]}$ is the compensated jump process expressed in \eqref{eq_219} which is an \break $L^2$-martingale, see for example \morecite{applebaum2009levy}. From Proposition \ref{levy_Ito_decop}, the L\'evy-Ito decomposition guarantees that every  $L^2$-L\'evy process has the representation 
\begin{equation}\label{eq_4}
X_t = \Sigma W_t + L_t + bt.
\end{equation}
Without loss of generality, we impose the following conditions on our variables for tractability of our numerical analysis. 
\begin{assumption}\label{Assumption_1}
There exist a constant $k \in \mathbb{R}_+$ such that 
\begin{enumerate}[label=(\roman*)]
        \item $\int_{\mathbb{R}^d} (1 \land |x|^2)  v(dx) \leq k^2$,
        \item $|\Sigma|\leq k$,
        \item $|b|\leq k$,
        \item $|y_0|\leq k$.
   \end{enumerate}
\end{assumption}
The following theorem, whose proof can be found in \morecite{rong2005theory} sets up the usual condition that guarantees the existence of a unique strong solution to (\ref{eq_major}).
\begin{theorem}[Section 3.1, \morecite{rong2005theory}]\label{Thm_2_1}
Consider the SDE driven by a \break square-integrable L\'evy process given in (\ref{eq_major}). Let $a : \mathbb{R}^d \to \mathbb{R}^{d \times d}$ be a measurable function such that 
\begin{equation*}
    |a(x) - a(x^{\prime})| \leq k^{\prime} |x - x^{\prime}| \qquad and \qquad |a(y_0)|\leq k^{\prime},
\end{equation*}
    for $x, \, x^{\prime} \, \in \, \mathbb{R}^d$ and $k^{\prime} \, \in \, \mathbb{R}_+$. Then, equation (\ref{eq_major}) has a unique strong solution adapted to the filtration generated by $X$ denoted by $\mathcal{F}^X := \sigma(X_t; \, t \geq 0)$, such that 
    \begin{equation*}
        \mathbb{E}[\sup_{t \in [0, T]}|Y_t|^2] \leq \mathbf{C}_1,
    \end{equation*}
    where $\mathbf{C}_1$ is a positive constant depending on $k^{\prime}$ and $T$ only.
\end{theorem}
\begin{remark}
Subsequently, for $a, b \in \mathbb{R}_+$ we denote by $a \lesssim b$ the fact that $a/b$ is uniformly bounded. We shall also denote constants depending on $k$ and $T$ by $(\mathfrak{c}_i)_{i \geq 0}$ and  $(\mathbf{C}_i)_{i \geq 0}$. Without loss of generality, we let $k^{\prime} = k$ in Theorem \ref{Thm_2_1}.
\end{remark}
%see stack extchange, the term uniformly bounded implies that a/b is not dependent on whatever they are bounde by. See https://math.stackexchange.com/questions/215065/what-is-the-difference-between-totally-bounded-and-uniformly-bounded
  %$\mathfrak{c}$ $\mathbf{C}$ 
\subsection{Examples of L\'evy processes}
\begin{example}[Brownian motion]\label{example_Brown}
A stochastic process $(W_t)_{t \geq 0}$ in $\mathbb{R}$ is called a Brownian Motion with variance $\Sigma$, if $B_1$ is normally distributed with mean 0 and sample paths of  $(W_t)_{t \geq 0}$ are almost surely continuous. This the only L\'evy process with continuous sample paths.
\end{example}
\begin{example}[The Poisson process]\label{example_Poi}
The Poisson process with intensity $\lambda > 0$, is a L\'evy process  $(N_t)_{t \geq 0}$ taking values in $\mathbb{N} \cup \{0\}$, where each $N_t$ follows a Poisson distribution with parameter $\lambda t$.
\end{example}
\begin{example}[The compound Poisson process]\label{example_Compound_Poi} 
Let $Y = (Y_n)_{n \in \mathbb{N}}$ be a sequence of i.i.d. random variables in $\mathbb{R}^d$ with common distribution $\mu_Y$. Let $N_t$ be the given Poisson process in Example \ref{example_Poi} independent of $Y_t$ for all $t \geq 0$. The process $Z$ defined as
\begin{equation*}
    Z_t = \sum_{k=1}^{N_t} Y_k, \qquad \qquad for \, all \quad t \geq 0,
\end{equation*}
is said to be a compound Poisson process.
\end{example}
\begin{example}[Jump-diffusion process]
Let $W$ be the Brownian Motion in Example \ref{example_Brown} and $Z$ the compound Poisson process in  Example \ref{example_Compound_Poi} then the process $(J_t)_{t \geq 0}$ given as 
\begin{equation}
    J_t = W_t +  Z_t,
\end{equation}
is a L\'evy process called a jump-diffusion process.
\end{example}
\begin{example}[Tempered stable family] A L\'evy Process $Z$ with the characteristic triplet $(b, \, 0 ,\, v)$ is said to belong to the Tempered stable family if its jump density is given by
\begin{equation}\label{eq_tempered}
    v(x) = c_+ \dfrac{e^{-\alpha_+ x}}{x^{\lambda_+}}\mathbbm{1}_{\{x > 0\}} + c_- \dfrac{e^{\alpha_- x}}{|x|^{\lambda_-}} \mathbbm{1}_{\{x < 0\}},
\end{equation}
where $c_{\pm} > 0$, $\lambda_{\pm} > 0$, $\alpha_{\pm} \in (0, 2)$. With appropriate choice of parameters, one obtains the Normal Inverse Gaussian (NIG) process, the CGMY process proposed by \morecite{carr2002fine} and the KoBoL process introduced by \morecite{koponen1995analytic}, see Example \ref{Example_betafam}.
\end{example}
\section{Meromorphic  L\'evy processes}\label{Sect_Merom}
The name Meromorphic derives from the fact that the characteristic exponent $\Psi$ in (\ref{inf_div_dist_levy}) can be extended to a meromorphic function (which may be understood as the ratio of two holomorphic functions) in $\mathbb{C}$. This class of meromorphic processes is rich, since the paths of bounded and unbounded variation as well as finite and infinite activity jumps can be generated (cf.\morecite{ferreiro2015applying}, Section 5.1).
\begin{definition}
A function $h:\mathbb{R}_+ \to \mathbb{R}$ is said to be \emph{completely monotone} if $h \in \mathcal{C}^\infty $ and $(-1)^n h^{(n)}(x) \geq 0$ for all $n \in \mathbb{N} \cup \{0\}$. Moreover, Bernstein's theorem states that $h$ is completely monotone if and only if it can be represented as Laplace transform of a positive measure $\mu$ on $[0, \infty)$:
\begin{equation}\label{eq_cm}
    h(x) = \int_0^\infty e^{-xz} \mu(dz),  \qquad x>0. 
\end{equation}
If $h:\mathbb{R}_+ \to \mathbb{R}$ in (\ref{eq_cm}) is discrete, it said to be a \emph{discrete completely monotone} function and can be represented as  
\begin{equation*}
    h(x) = \sum_{n \geq 1} a_n e^{-b_n x}, \qquad x>0.
\end{equation*}
where $a_n >0$, $b_n \geq 0$, $\lim_{n \to \infty} b_n = \infty$ and the sequence $(b_n)_{n \geq 1}$, assumed to be strictly increasing.
\end{definition}
\begin{remark}
\begin{enumerate}[label=(\roman*)]
    \item[]
    \item If $h$ is \emph{completely monotone}, whose class we denote $\mathcal{C} \mathcal{M}$; and   $h(0^+) = 1$, then $\mu(dz)$ is a probability measure and $1-h(x)$ is the cumulative distribution function of a positive infinitely divisible random variable $X$, whose distribution is mixture of exponential distributions (cf. \morecite{kuznetsov2012meromorphic} Section 2).
    \item The class of discrete completely monotone functions is denoted by $\mathcal{D}\mathcal{C} \mathcal{M}$.
\end{enumerate}

\end{remark}
\begin{definition}
A L\'evy process $X$ is said to belong to the meromorphic class if its L\'evy measure $v$ decomposes as 
\begin{equation*}
    v^+(x) = v((x, \infty)) \quad \text{and} \quad  v^- = v((-x, -\infty)), \qquad for \, all\quad x > 0,
\end{equation*}
and $v^+, v^- \in \mathcal{D}\mathcal{C} \mathcal{M}$.
\end{definition}
\subsection{Examples of meromorphic  L\'evy processes}\label{ibr}
\begin{example}[The compound Poisson process] See Example \ref{example_Compound_Poi}.
\end{example}

\begin{example}[$\beta$-family of L\'evy processes]\label{Example_betafam}
A L\'evy Process $Z =(Z_t)_{t \geq 0}$ in $\mathbb{R}^d$ with the generating triplet $(b, \, \Sigma,\, v)$ is said to belong to the $\beta$-family if its jump density is given by
\begin{equation}\label{meromorphic}
    v(x) = c_1 \dfrac{e^{-\alpha_1 \beta_1 x}}{(1 -e^{-\beta_1 x})^{\lambda_1}}\mathbbm{1}_{\{x > 0\}} + c_2 \dfrac{e^{\alpha_2 \beta_2 x}}{(1 -e^{\beta_2 x})^{\lambda_2}}\mathbbm{1}_{\{x < 0\}},
\end{equation}
where $\alpha_i, \beta_i > 0 $, $c_i \geq 0$, $\lambda_{i} \in (0, 3)$, $i = 1, 2$; with L\'evy-Khintchine representation given as 
\begin{equation*}
    \Phi(\vartheta) = \dfrac{\Sigma^2}{2}\vartheta + i \rho \vartheta - \dfrac{c_1}{\beta_1}\mathcal{K} \bigg(\alpha_1 - \dfrac{i\vartheta}{\beta_1}, 1 - \lambda_1 \bigg) - \dfrac{c_2}{\beta_2}\mathcal{K} \bigg(\alpha_2 - \dfrac{i\vartheta}{\beta_2}, 1 - \lambda_2 \bigg) + \varrho,
\end{equation*}
where 
\begin{equation*}
    \begin{aligned}
         \varrho &= \dfrac{c_1}{\beta_1}\mathcal{K} (\alpha_1, 1 - \lambda_1) + \dfrac{c_2}{\beta_2}\mathcal{K}(\alpha_2, 1 - \lambda_2),\\
         \rho &= \dfrac{c_1}{\beta_2^2}\mathcal{K}(\alpha_1, 1 - \lambda_1) (\mathcal{K}^\prime(1 + \alpha_1 - \lambda_1) - \mathcal{K}^\prime(\alpha_1 ))\\
         &\qquad - \dfrac{c_2}{\beta_2^2}\mathcal{K}(\alpha_2, 1 - \lambda_2) (\mathcal{K}^\prime(1 + \alpha_2 - \lambda_2) - \mathcal{K}^\prime(\alpha_2)) - b,
    \end{aligned}
\end{equation*}
with $b, \Sigma \in \mathbb{R}$, $\mathcal{K}(x, y) = \Gamma(x) \Gamma(y) / \Gamma(x + y)$ and $\mathcal{K}^\prime(x) = \frac{d}{dx}\log\Gamma(x)$. $\Gamma(\cdot)$ is the conventional Gamma function.\\
If in \eqref{meromorphic} we let $\beta \to 0^+$ and let $c_1 = c_+ \beta^{\lambda_+}$, $c_- = c_-\beta^{\lambda_-}$, $\alpha_1 = \alpha_+ \beta^{-1}$,  $\alpha_2 = \alpha_- \beta^{-1}$, $\beta_1 = \beta_2 = \beta$; one obtains the generalized Tempered stable family. Particularly,
\begin{enumerate}[label=(\roman*)]
    \item if $\lambda_{1} = \lambda_{2}$, the resulting process is called KoBoL process. 
    \item If $c_1 = c_2$, $\lambda_{1} = \lambda_{2}$ and $ \beta_1 =  \beta_2$, the jump density converges to that of CGMY processes.
    \item If $c_1 = c_2 = 4$, $ \beta_1 =  \beta_2 = 1/2$,  $\lambda_{1} = \lambda_2 = 2$, $\alpha_1 = 1- \alpha$, $\alpha_2 = 1 + \alpha$, one obtains an analogue of the NIG process. 
\end{enumerate}
\begin{remark}
For a detailed treatment and simulations of the given examples, see  \morecite{ferreiro2015applying}, \morecite{kuznetsov2012meromorphic} and \morecite{kuznetsov2011wiener}.
\end{remark}

\end{example}
\section{Arrival Time of the Poisson Process}\label{Section2_2}
The Poisson process is an example of a stochastic process with discontinuous paths. It has been used by a wide variety of authors such as, \morecite{tankov2003financial, sato1999levy, protter2005stochastic}, as a basis for building more complex jump processes. Furthermore, the major task of the numerical analysis in the proposed scheme relies on the interplay between the arrival and interarrival time of the Poisson process.

\begin{definition}[Exponential distribution] A continuous random variable $\xi$ is said to have an exponential distribution with parameter $\lambda > 0$, written as $\xi \sim Exponential(\lambda)$, if its probability density function is given by 
\begin{equation}
    f_\xi(x) = \lambda e^{\lambda x}\mathbbm{1}_{x\geq 0}.
\end{equation}
Moreover, the distribution function of $X$ is given by 
\begin{equation}
    F_\xi(x) = \mathcal{P}(\xi \leq x) = 1 - e^{-\lambda x}, \qquad \forall x \in [0, \, \infty),
\end{equation}
with the $n$-th moment of $\xi$ for any $n \in \mathbb{N}$ being
\begin{equation*}
    \mathbb{E}[\xi^n] = \dfrac{n!}{\lambda^n}.
\end{equation*}
\end{definition}
\begin{definition}[Poisson Process]\label{POisson_1}
A counting process $(\mathcal{N}_t)_{t \geq 0}$ is said to be a Poisson process with rate  $\lambda > 0$ if the following conditions are satisfied
\begin{enumerate}[label=(\roman*)]
     \item $\mathcal{N}_0$ = 0 a.s.
     \item $\mathcal{N}_t$ has independent increments, that is, for any $t_1 < \cdots < t_n < \infty $, \break $\mathcal{N}_{t_2} - \mathcal{N}_{t_1}, \, \ldots, \mathcal{N}_{t_n} - \mathcal{N}_{t_{n-1}}$ are independent.
     \item The number of events in any time interval of length $t > 0$ is Poisson distributed with mean $\mathbb{E}[\mathcal{N}_t] = \lambda t$. That is,
     \begin{equation}\label{pdf_poisson}
         \mathcal{P} (N_t = \kappa) = \dfrac{(\lambda t)^\kappa}{\kappa !}e^{-\lambda t}\mathbbm{1}_{ \kappa > 0}.
     \end{equation}
\end{enumerate}
\end{definition}
The next closely related distribution is the Gamma distribution which could be viewed as a generalization of an exponential distribution.
\begin{definition}[Gamma distribution]
A continuous random variable $X$ is said to have a Gamma distribution with shape parameter $\alpha > 0$ and rate parameter $\lambda > 0$, written as $X \sim \Gamma(\alpha, \lambda)$, if its probability density function is given by  
\begin{equation}
    f_X(x) = \dfrac{\lambda^{\alpha} x^{\alpha-1} e^{-\lambda x}}{\Gamma (\alpha)}\mathbbm{1}_{x\geq 0},
\end{equation}
where $\Gamma (\cdot)$ is the Gamma function.
\end{definition}
\begin{remark}\label{remark_Gamma}
\begin{enumerate}[label=(\roman*)]
        \item[]
        \item If $\alpha = 1$, we recover an exponentially distributed random variable, i.e., \break $ \Gamma(1, \lambda) \overset{d}{=}  Exponential(\lambda)$.
        \item One could also show by induction that the sum $X = \sum_{i=1}^n \xi_i $ of independent random variables following an exponential distribution, that is,  $\xi_i \sim Exponential(\lambda)$, is equal in distribution to a Gamma distribution, that is, $X \sim \Gamma(\alpha,\lambda)$, $\alpha, \lambda > 0$.
   \end{enumerate}
\end{remark}
The relationship between the Exponential distribution and the Poisson process is captured in the following proposition.
\begin{proposition}[Section 2.5, p.47 \morecite{tankov2003financial}]\label{prop_Poisson}
If $(T_i)_{i \geq 1}$ are independent exponential random variables with rate $\lambda > 0$ then, for any $t >0 $, the random variable 
\begin{equation}
   \mathcal{N}_t = \inf \{k \geq 1; \sum_{i = 1}^k T_i > t\}
\end{equation}
follows a Poisson distribution with rate $\lambda t$, i.e.,
 \begin{equation}       
        \mathcal{P} (\mathcal{N}_t = \kappa) = \dfrac{(\lambda t)^\kappa}{\kappa !}e^{-\lambda t}\mathbbm{1}_{ \kappa > 0}.     
 \end{equation}
 \end{proposition}
\begin{proof}
Let $\mathcal{T}_n = \sum_{i = 1}^n T_i $ for all $n$. The density of $(\mathcal{T}_1, \ldots \mathcal{T}_n)$ is given by 
\begin{equation*}
    \lambda^n \One_{0 < t_1 < \cdots < t_n}e^{-\lambda t_n} dt_1 \ldots dt_n.
\end{equation*}
Since $\mathcal{P} (\mathcal{N}_t = \kappa) = \mathcal{P}(t \in [\mathcal{T}_\kappa,  \mathcal{T}_{\kappa+1}))$, it can be estimated as 
\begin{equation*}
    \begin{split}
        \mathcal{P} (\mathcal{N}_t = \kappa) &= \int_{0 < t_1 < \cdots < t_\kappa< t< t_{\kappa+1}}\lambda^\kappa e^{-\lambda t_{\kappa +1}} dt_1 \ldots dt_\kappa dt_{\kappa +1}\\
        &= \lambda^\kappa e^{-\lambda t} \int_{0 < t_1 < \cdots < t_\kappa< t} dt_1 \ldots dt_\kappa\\
        &= \dfrac{(\lambda t)^\kappa}{\kappa !}e^{-\lambda t}.
    \end{split}
\end{equation*}
\end{proof}
\section{The discretization scheme}
For $n \geq 1 ,$ let $ \xi (n/T) := (\xi_{i} (n/T))_{i \geq 1}$  be a sequence on independent identically distributed (i.i.d) random variables defined on the probability space  $(\Omega, \mathcal{F}, \mathcal{P}),$ where each $\xi_i$ is exponentially distributed and dependent on rate $n/T$. For this we write \break  $\xi_{i} \sim Exponential(n/T)$, where $\mathbb{E}[ \xi_{i}]= T/n $. We denote further by $\mathcal{F}^{\xi} := \sigma( \xi_{i} ; \,  i \geq 1)$ the  $\sigma$-algebra generated by $ \xi $, which is assumed to be independent of $X$, and set  $\xi_{0} = 0$. %and omit the dependence of $\xi $ on $(n/T)$ for simplicity of notation.

We recall that the Euler approximation of a stochastic differential equations is given on a time grid $0 = t_0 < t_1 < \ldots < t_n$ by
\begin{equation*}
Y^*_{t_{i}} := Y^*_{t_{i-1}} + a(Y^*_{t_{i-1}})\Delta W_{t_{i-1}},  \qquad \qquad
  Y^*_0 = y_0, \qquad \qquad  i = 1, 2, \ldots n-1,
\end{equation*}
where $a : \mathbb{R}^d \to \mathbb{R}^{d \times d}$ is a measurable function, $\Delta W_{t_{i-1}} := W_{t_{i}}-W_{t_{i-1}}$ is the increments of the Wiener process $W$ and $\Delta W_{t_{i}} \sim N (0, \, \Delta_{t_{i}})$, i.e., the increment follows an independent Gaussian distribution; whereas, the Euler-Poisson scheme is then given by the sequence $\tilde{Y} := (\tilde{Y}_{t_i})_{i\geq 0}$ defined as 
\begin{equation}\label{eq5}
\tilde{Y}_{t_{i}} := \tilde{Y}_{t_{i-1}} + a(\tilde{Y}_{t_{i-1}})\Delta X_{\xi_{i}(n/T)},  \qquad \qquad
  \tilde{Y}_0 = y_0, \qquad \qquad  i = 1, 2, \ldots .
\end{equation}
with independent and stationary increment $\Delta X_{\xi_{i}(n/T)} :=  X_{\xi_{i}(n/T)} - X_{\xi_{i-1}(n/T)} \overset{d}{=}  X_{\xi (n/T)}$. We define the random grid $(t_{i})_{i \geq 0}$ by the partial sum of sequences of these $\xi_{i}$'s, specified as
\begin{equation}\label{t_i}
 t_{i} := \sum_{j=0}^{i}  \xi_{j}(n/T),
\end{equation}
and $$\mathcal{N}(n/T) = \mathcal{N} := (\mathcal{N}_{t})_{t \geq 0}$$ is the Poisson process with the arrival times $(t_{i})_{i \geq 0}$. It is noteworthy that $t_{i}$ follows a Gamma distribution with shape parameter $i$ and rate parameter $n/T$, i.e., $t_{i} \overset{d}{=} \Gamma(i, \, n/T)$. The mean $\mathbb{E}[\xi_{i}]= T/n$ corresponds to the time-steps of the deterministic equally-spaced Euler scheme. Based on the foregoing construction, the  claim here is that $\tilde{Y}_{t_n}$ is an approximation of the solution $Y$ at end point $T$ and the aim of this work is to derive the asymptotic behavior of 
\begin{equation}\label{eq6}
\lim_{n\to\infty} \mathbb{E}[|Y_T - \tilde{Y}_{t_n}|^2].
\end{equation}

In order to carry out the numerical analysis, an interpolant, which stochastically interpolates the Euler-Poisson scheme is introduced: let $\iota(t)$ denote the largest grid point before $t$ in this scheme, expressed as:
\begin{equation*}
\iota(t) := \sup[0, \, t] \cap (t_{i})_{i \geq 0}
\end{equation*}
and define 
\begin{equation}\label{eq7}
\hat{Y}_{t} := y_0 + \int^t_{0} a(\hat{Y}_{\iota(s-)})dX_s = \hat{Y}_{\iota(t)} + a(\hat{Y}_{\iota(t)})(X_t - X_{\iota(t)}), \qquad t \in [0, t_n \lor T].
\end{equation}
Observe that for $t \in [t_i, \, t_{i+1})$ we have $\hat{Y}_{t_i}= \tilde{Y}_{t_i} = Y_{\iota(t)}$, i.e., the processes coincide almost surely for all random times $(t_{i})_{t \geq 0}$. Therefore, $ \hat{Y} = (\hat{Y})_{t \in [0, t_n \lor T]} $ interpolates the process $\tilde{Y}$ in a random way. Furthermore, another important variable crucial to the derivations carried out in this work is the largest distance on the random grid $(t_{i})_{t \geq 0}$ restricted to $[0, T]$. This $\mathcal{F}^{\xi}$-measurable random variable is denoted 
\begin{equation}\label{eq8}
\tau := \sup_{s \in [0, T] } (s - \iota (s)).
\end{equation}
\subsection{ The Moments of $\tau$}
In the classical Euler scheme, the maximum time step is bounded by a deterministic finite constant, however, this cannot be the case for the Euler-Poisson scheme since our time steps are generated by a the arrival times Poisson process. Thus in the following, we give the moments of $\tau$ following the expositions of \morecite{ferreiro2016euler}. 

For a Poisson process $\mathcal{N}$, conditioned on $d$ arrivals occurring in the interval from 0 up to and including $T$, the timing of the arrivals have the same distribution as $d$ ordered independent uniformly distributed random variables on $[0, \, T]$. Since $\tau$ is defined in (\ref{eq8}) to be the largest random gap between two neighbouring points, an approach would be to study the maximum gap on the unit interval $[0, \, 1]$ defined by these $d$ ordered independent uniformly distributed random variables using the well known principle of order statistics.
\begin{definition}
For $d > 0$, let $\{D_i\,; i = 1, \ldots, d-1 \}$ be a sequence of i.i.d. random variables where each random variable $D_i$ follows a uniform distribution on $[0 \,, 1]$. We say that $\{D_{(i)}\,; i = 0, \ldots, d \}$ is the order statistic corresponding to $\{D_i\,; i = 1, \ldots, d-1 \}$ if $D_{(k)}$ is the $k$-th smallest value among $\{D_i\,; i = 1, \ldots, d-1 \}$, $k = 0, \ldots, d$ where $D_0 =0$ and $D_d =1$. The value 
\begin{equation}\label{largest_gap}
    \Lambda_{d} := \max_{i = 1, \ldots, d}\{D_{(i)} - D_{(i-1)}\},
\end{equation}
is said to be the largest gap.
\end{definition}
A comparison of the definition of $\tau$ and (\ref{largest_gap}) reveals that $\dfrac{1}{T}\tau$ conditioned on $\mathcal{N}_T$,is equal in distribution to $\Lambda_{\mathcal{N}_T+1}$, and hence
\begin{equation}\label{exp_tau}
    \dfrac{1}{T}\mathbb{E}[\tau] = \mathbb{E}[\Lambda_{\mathcal{N}_T+1}].
\end{equation}
For an overview of the behavior of $ \Lambda_{d}$ we refer the reader to \morecite{fisher1929tests}. The following equation was taken from \morecite{mauldon1951random} 
\begin{equation*}
    \mathbb{E}[(1-\Lambda_{d}s)^{-d}] = \dfrac{d!}{1-s} \prod_{j=2}^d \dfrac{1}{j-s}, \qquad |s|< 1/2, d \geq 1.
\end{equation*}
For $d \geq 1$ we have that
\begin{equation*}
     \mathbb{E}[\Lambda_{d}] = \dfrac{\sum_{j=1}^d\frac{1}{j}}{d} = \dfrac{\digamma (d+1)+ \nu}{d},
\end{equation*}
where $\digamma$ is the digamma function (cf. \morecite{stegun1970handbook}, Sections 6.3.2 and 6.4.10).
We note that 
\[
 (\digamma (d+1)+ \nu)
  \begin{cases} 
   = 0 & \text{for } d = 0 \\
   > 0  & \text{for } d > 0 \\
   = \log(d+1) & \text{for } d \to \infty.
  \end{cases}
\]
It follows that $\lim_{d \to 0}\dfrac{\digamma (d+1)}{\log(d+1)} = 1$, and we thus conclude that there is a constant $\mathfrak{c}_0$ independent of $d$ such that $\digamma (d+1)+ \nu \leq \mathfrak{c}_0 \log(d+1) $. Therefore
\begin{equation*}
     \mathbb{E}[\Lambda_{d}] = \mathfrak{c}_0 \dfrac{\log(d+1)}{d} \qquad for \, d = 1, 2, \ldots
\end{equation*}
\begin{proposition}\label{Mom_tau}
Using the facts above, it holds that 
\begin{equation*}
    \mathbb{E}[\tau] + \mathbb{E}[\tau^2] \lesssim \dfrac{\log(n)}{n}.
\end{equation*}
\end{proposition}
\begin{proof}
(\ref{exp_tau}) combined with the given arrival rate $n/T$ for the Poisson process $\mathcal{N}$ yields
\begin{equation*}
    \begin{split}
        \dfrac{1}{T}\mathbb{E}[\tau] &= \mathbb{E}[\Lambda_{\mathcal{N}_T+1}]= \sum_{\kappa=0}^\infty \mathbb{E}[\Lambda_{\mathcal{N}_T+1} | \mathcal{N}_T] \mathcal{P} (\mathcal{N}_t = \kappa)\\
        &\leq \mathfrak{c}_0 \sum_{\kappa=0}^\infty \dfrac{\log(\kappa+1)}{\kappa+1}\exp{\big(-\dfrac{n}{T}\big)}\dfrac{(n/T)^\kappa}{\kappa!}\\
        &= \dfrac{\mathfrak{c}_0 T}{n} \sum_{\kappa=0}^\infty \log(\kappa+1)\exp{\big(-\dfrac{n}{T}\big)}\dfrac{(n/T)^\kappa}{\kappa!}\\
        &= \dfrac{\mathfrak{c}_0 T}{n}\mathbb{E}[\log(\mathcal{N}_T+1)].
    \end{split}
\end{equation*}
By the concavity of $x \mapsto \log (x + 1)$ for $x \in [0, \infty)$ and Jensen's inequality, one has that 
\begin{equation*}
    \begin{split}
         \dfrac{1}{T}\mathbb{E}[\tau] &= \mathbb{E}[\Lambda_{\mathcal{N}_T+1}] \leq \dfrac{\mathfrak{c}_0 T}{n}\log(\mathcal\mathbb{E}[{\mathcal{N}}_T]+1)\\
         &= \dfrac{\mathfrak{c}_0 T \log(\frac{n}{T}+1)}{n}.
    \end{split}
\end{equation*}
The assertion follows from the fact that, for $\Lambda_{d} \in [0, 1]$, $\Lambda_{d}^2 \leq \Lambda_{d}$, and, thus,
\begin{equation*}
     \dfrac{1}{T^2}\mathbb{E}[\tau^2] = \mathbb{E}[\Lambda^2_{\mathcal{N}_T+1}] \leq \mathbb{E}[\Lambda_{\mathcal{N}_T+1}]
\end{equation*}
\end{proof}
\subsection{Main Result and Feasibility of the Euler-Poisson scheme}\label{Section2_3}
Having presented the preliminaries and notations, we now proceed to a formal statement of the main result of this thesis.
\begin{theorem}\label{main_result}
Under the assumption of Theorem \ref{Thm_2_1} we have that 
\begin{equation*}
    \mathbb{E} [|Y_{T} - \tilde{Y}_{t_{n}}|^2] \leq \mathbf{C}_2\sqrt{\dfrac{1}{n}},
\end{equation*}
where $\mathbf{C}_2 $ depends on $k$ and $ T$ only, $\mathbf{C}_2, k \in \mathbb{R}_+, T < \infty$.
\end{theorem}
The Euler-Poisson scheme would benefit from the possibility of sampling from the distribution of $X_{\xi}$, which in general would require the same level of technicality as sampling from that of $X_{1}$. Thanks to the contributions of \morecite{kuznetsov2011wiener},  \morecite{kuznetsov2010wiener} and \morecite{kuznetsov2012meromorphic} to the recent developments in Wiener-Hopf factorization theory for 1-dimensional L\'evy processes, the authors provide one with a handful of examples for which distributional sampling can be performed. Moreover, one could infer from their discussions that the Euler-Poisson scheme is a possible simpler numerical technique for (\ref{eq_major}). The class of processes for which the distributional sampling can be performed is named meromorphic L\'evy processes. \morecite{ferreiro2016euler} claims that the Wiener-Hopf factorization gives a lot more information than  is needed to implement the Euler-Poisson scheme, as it involves the running infimum and running supremum of the stochastic process $X$.
\begin{definition}
Let $X$ be a L\'evy process and $\xi \sim Exponential(n/T)$ such that $\mathbb{E}[ \xi]= T/n > 0$. The following processes
\begin{equation*}
  \mathcal{S}_t = \sup \{ X_s \,; s\in [0, t]\}, \qquad  \mathcal{I}_t = \inf \{ X_s \,; s\in [0, t]\}
\end{equation*}
are called running supremum and running infimum of $X$, respectively. 
\end{definition}
We drop the dependence of $\xi $ on $(n/T)$ as usual. The Wiener-Hopf factorization states that the random variables $\mathcal{S}_{\xi}$ and $X_{\xi} - \mathcal{S}_{\xi}$ are independent. Further, due to the equality in distribution of $(X_{s} - X_{(t-s)-})_{s\in [0, t]}$ and $(X_{s})_{s\in [0, t]}$, it follows that $\mathcal{I}_{\xi} \overset{d}{=} X_{\xi} - \mathcal{S}_{\xi} $. The result is the following  characteristic exponent factorization known as the Wiener-Hopf factorization:
\begin{equation}\label{WH_fact}
    \mathbb{E}[e^{i \vartheta X_{\xi}}] = \mathbb{E}[e^{i \vartheta \mathcal{S}_{\xi}}] \times \mathbb{E}[e^{i \vartheta \mathcal{I}_{\xi}}], \qquad \forall \, \vartheta \in \mathbb{R}.
\end{equation}
The Wiener-Hopf factors are explicit for the class of meromorphic L\'evy processes. Authors such as \morecite{ferreiro2015applying}, \morecite{kuznetsov2010wiener} and \break \morecite{kuznetsov2011wiener} have shown that one can effectively sample from the law of $X_\xi$ through (\ref{WH_fact}) for  simulating of a variety of L\'evy processes. For instance, \morecite{ferreiro2014multilevel} applied the Multilevel Monte Carlo simulation technique to the $\beta$-family pf meromorphic L\'evy processes and concludes that the numerical algorithms involving the computation of $X_\xi$ for such processes are very easy to implement and robust with respect to the jump structure. The $\beta$-family of meromorphic L\'evy processes (cf. \Cref{ibr}) offers the desirable properties similar to those in used  mathematical finance; more so, \morecite{kuznetsov2010wiener} argues that a large class of L\'evy processes can be approximated by a member of the $\beta$-class and a compound Poisson process. In this light, \morecite{schoutens2011beta} examined the numerical performance of the $\beta$-family of meromorphic L\'evy processes with parameters chosen such that the L\'evy density is approximately equal to that of the classical Variance Gamma (VG) model. The resulting model referred to as $\beta$-VG model was found to track the original model, even though the $\beta$-VG required more computation time. Likewise, \morecite{ferreiro2012beta} employed the same methodology to obtain a $\beta$-family analog of the Meixner model and the result was congruent to that of \morecite{schoutens2011beta}. This brings the possibility to study new processes associated to the stochastic differential equation (\ref{eq_major}). For instance, the following model
\begin{equation}\label{Pop_Dy}
      Y_t = y_0  + \int_0^t a(Y_{s-}, \mathcal{S}_{s-})dX_s  \quad \text{or} \quad  Y_t = y_0  + \int_0^t a(Y_{s-}, \mathcal{I}_{s-})dX_s \qquad t \in [0, \, T].
\end{equation}
can be found in \morecite{ferreiro2016euler}. The proposition is that, the stochastic dynamics of populations (see expositions of \morecite{rong2005theory},  Chapter 11) or chemical reactions can be modelled using (\ref{Pop_Dy}), where the knowledge of $\mathcal{S}$ can replace the artificial barrier restrictions that are usually imposed on the driving process due to physical constraints.

%It is noteworthy that in the literature, that the Wiener-Hopf factorization is only applicable to L\'evy processes in one dimension; that notwithstanding, \morecite{ferreiro2016euler} asserts the possibility of the application of the Euler-Poisson scheme for multidimensional L\'evy processes $X$, given that they are of the form $\mathbf{K}\widehat{X}$, where $\mathbf{K} \in \mathbb{R}^{d \times d}$ and $\widehat{X} := (\widehat{X}_t)_{t \geq 0}$ is a $d$-dimensional L\'evy process with independent components $(\widehat{X}_i)_{i = 1, \ldots, d}$ such that $\widehat{X}_i$ belongs to the Meromorphic class for $i \in [1, d]$. Due to componentwise independence, we can perform Wiener-Hopf factorization in each $(\widehat{X}_i)_{i = 1, \ldots, d}$ and obtain the distribution of $\widehat{X}_\xi$ so that  $X_\xi \overset{d}{=}\mathbf{K}\widehat{X}_\xi$.With this construction, the dependence between the components of $X$ is limited to a correlation matrix. Contrarily multidimensional L\'evy processes suffer from the difficulty of handling the with respect to simulation relative to their 1-dimensional counterparts for which only a few constructions allow for efficient numerical simulation of these multidimensional L\'evy processes. \morecite{ferreiro2016euler} proposes that given the above construction, one could assemble  a multidimensional L\'evy process with a matrix correlation with a general L\'evy process in their marginals, in which case one confronts same sampling issues with general 1-dimensional L\'evy processes. A second approach is to perform a univariate time change in a multidimensional Brownian motion, achieving generalized versions of many popular models employed in finance such as VG, the Normal Inverse Gaussian (NIG) or the Cox-Ingersoll-Ross (CIR) process, which generally do not allow for imposition of independent components' subsets. For random vectors, it is possible to characterize the dependence between components in terms of the marginals by means of copula, but this techniques becomes more involved when it come to stochastic processes (cf. \morecite{tankov2003financial} Chapter 5.3 and \morecite{kallsen2006characterization}) and it is unclear how to numerically analyse such an approach.
